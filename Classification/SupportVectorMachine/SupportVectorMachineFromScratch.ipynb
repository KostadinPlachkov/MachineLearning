{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SupportVectorMachine:\n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1: \"r\", -1: \"b\"}\n",
    "        if self.visualization:\n",
    "            self.figure = plt.figure()\n",
    "            self.axis = self.figure.add_subplot(1, 1, 1)\n",
    "    # Train\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        # { ||w||: [w, b]}\n",
    "        opt_dict = []\n",
    "        transforms = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "        all_data = []\n",
    "        for yi in self.data:\n",
    "            for feature_set in self.data[yi]:\n",
    "                for feature in feature_set:\n",
    "                    all_data.append(feature)\n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "        \n",
    "        # Support vectors yi(xi.w + b) = 1\n",
    "        # You will know that you have found a really great value for w and b, when in both positive and negative classes you have a value close to 1.\n",
    "        \n",
    "        step_size = [self.max_feature_value * 0.1,\n",
    "                    self.max_feature_value * 0.01,\n",
    "                     # Point of expense:\n",
    "                    self.max_feature_value * 0.001]  # Add self.max_feature_value * 0.0001 to be more preciseÐ´\n",
    "        # Extremely expensive\n",
    "        b_range_multiple = 5\n",
    "        # No need to take as small of steps with b as w\n",
    "        b_multiple = 5\n",
    "        latest_optimum = self.max_feature_value * 10\n",
    "        \n",
    "        for step in step_size:\n",
    "            w = np.array([latest_optimum, latest_optimum])\n",
    "            # Convex\n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1 * self.max_feature_value * b_range_multiple,\n",
    "                                   self.max_feature_value * b_range_multiple,\n",
    "                                   step * b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_transform = w * transformation\n",
    "                        found_option = True\n",
    "                        # Weakest link in the SVM fundamentally\n",
    "                        # SMO attempts to fix this bit\n",
    "                        # yi(xi.w + b) >= 1\n",
    "                        for i in self.data:  # i = the class\n",
    "                            for xi in self.data[i]:\n",
    "                                yi = i\n",
    "                                # Even if one sample does not fit the definition, the whole thing is thrown out\n",
    "                                if not yi * (np.dot(w_transform, xi) + b) >= 1:\n",
    "                                    found_option = False\n",
    "                                    # break\n",
    "                            # break\n",
    "                        if found_option:\n",
    "                            opt_dict[np.linalg.norm(w_transform)] = [w_transform, b]  # The magnitude of the vector\n",
    "                if w[0] < 0:\n",
    "                    optimized =True\n",
    "                    print(\"Optimized a step.\")\n",
    "                else:\n",
    "                    w = w - step\n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            # ||w|| : [w, b]\n",
    "            opt_choice = opt_dict[norms[0]]  # The smallest norm\n",
    "            self.w = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0] + step * 2\n",
    "    def predict(self, features):\n",
    "        # sign(x.w + b)\n",
    "        classification = np.sign(np.dot(np.array(features), self.w) + self.b)\n",
    "        return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {-1: np.array([[1, 7], [2, 8], [3, 8]])\n",
    "            ,1: np.array([[5, 1], [6, -1], [7, 3]])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
